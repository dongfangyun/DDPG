    <!DOCTYPE html>
<html lang="zh-CN"><head><meta charset="UTF-8"><style>.nodata  main {width:1000px;margin: auto;}</style></head><body class="nodata " style=""><div class="main_father clearfix d-flex justify-content-center " style="height:100%;"> <div class="container clearfix " id="mainBox"><main><div class="blog-content-box">
<meta name="referrer" content="no-referrer">
<link rel="stylesheet" type="text/css" href="https://2.lz787.com/assets/1.css">

        <link rel="stylesheet" type="text/css" href="https://csdnimg.cn/release/blogv2/dist/pc/themesSkin/skin-yellow/skin-yellow-28d34ab5fa.min.css"><div class="blog-content-box">
    <div class="article-header-box">
        <div class="article-header">
            <div class="article-title-box">
                <h1 class="title-article" id="articleContentId">enable anomaly detection to find the operation that failed to compute its gradient, with torch.autog</h1>
            </div>
            <div class="article-info-box">
                <div class="article-bar-top">
                    <img class="article-type-img" src="https://csdnimg.cn/release/blogv2/dist/pc/img/original.png" alt="">
                    <div class="bar-content">
                      <a href="https://mall.csdn.net/vip" data-report-query="spm=3001.10404"  data-report-click='{"spm":"3001.10404"}' data-report-view='{"spm":"3001.10404"}'  class="article-vip-box" target="_blank"><img class="article-vip-img-new" src="https://csdnimg.cn/release/blogv2/dist/pc/img/identityVipNew.png" alt=""></a>
                      <a class="follow-nickName vip-name" href="https://xiaowutongzhi.blog.csdn.net" target="_blank" rel="noopener" title="静静的喝酒">静静的喝酒</a>
                    <img class="article-time-img article-heard-img" src="https://csdnimg.cn/release/blogv2/dist/pc/img/newUpTime2.png" alt="">
                    <span class="time">已于&nbsp;2022-07-26 12:49:36&nbsp;修改</span>
                   <div class="read-count-box">
                      <img class="article-read-img article-heard-img" src="https://csdnimg.cn/release/blogv2/dist/pc/img/articleReadEyes2.png" alt="">
                      <span class="read-count">阅读量7.3k</span>
                      <a id="blog_detail_zk_collection" class="un-collection" data-report-click='{"mod":"popu_823","spm":"1001.2101.3001.4232","ab":"new"}'>
                          <img class="article-collect-img article-heard-img un-collect-status isdefault" style="display:inline-block" src="https://csdnimg.cn/release/blogv2/dist/pc/img/tobarCollect2.png" alt="">
                          <img class="article-collect-img article-heard-img collect-status isactive" style="display:none" src="https://csdnimg.cn/release/blogv2/dist/pc/img/tobarCollectionActive2.png" alt="">
                          <span class="name">收藏</span>
                          <span class="get-collection">
                              43
                          </span>
                      </a>
                      <div class="read-count-box is-like">
                        <img class="article-read-img article-heard-img" style="display:none" id="is-like-imgactive-new" src="https://csdnimg.cn/release/blogv2/dist/pc/img/newHeart2023Active.png" alt="">
                        <img class="article-read-img article-heard-img" style="display:block" id="is-like-img-new" src="https://csdnimg.cn/release/blogv2/dist/pc/img/newHeart2023Black.png" alt="">
                        <span class="read-count" id="blog-digg-num">点赞数
                            31
                        </span>
                      </div>
                    </div>
                  </div>
                </div>
                <div class="blog-tags-box">
                    <div class="tags-box artic-tag-box">
                            <span class="label">分类专栏：</span>
                                <a class="tag-link" href="https://blog.csdn.net/qq_34758157/category_11936111.html" target="_blank" rel="noopener">pytorch</a>
                            <span class="label">文章标签：</span>
                                <a rel="nofollow" data-report-query="spm=1001.2101.3001.4223" data-report-click='{"mod":"popu_626","spm":"1001.2101.3001.4223","strategy":"深度学习","ab":"new","extra":"{\"searchword\":\"深度学习\"}"}' data-report-view='{"mod":"popu_626","spm":"1001.2101.3001.4223","strategy":"深度学习","ab":"new","extra":"{\"searchword\":\"深度学习\"}"}' class="tag-link" href="https://so.csdn.net/so/search/s.do?q=深度学习&amp;t=all&amp;o=vip&amp;s=&amp;l=&amp;f=&amp;viparticle=&amp;from_tracking_code=tag_word&amp;from_code=app_blog_art" target="_blank" rel="noopener">深度学习</a>
                                <a rel="nofollow" data-report-query="spm=1001.2101.3001.4223" data-report-click='{"mod":"popu_626","spm":"1001.2101.3001.4223","strategy":"pytorch","ab":"new","extra":"{\"searchword\":\"pytorch\"}"}' data-report-view='{"mod":"popu_626","spm":"1001.2101.3001.4223","strategy":"pytorch","ab":"new","extra":"{\"searchword\":\"pytorch\"}"}' class="tag-link" href="https://so.csdn.net/so/search/s.do?q=pytorch&amp;t=all&amp;o=vip&amp;s=&amp;l=&amp;f=&amp;viparticle=&amp;from_tracking_code=tag_word&amp;from_code=app_blog_art" target="_blank" rel="noopener">pytorch</a>
                                <a rel="nofollow" data-report-query="spm=1001.2101.3001.4223" data-report-click='{"mod":"popu_626","spm":"1001.2101.3001.4223","strategy":"神经网络","ab":"new","extra":"{\"searchword\":\"神经网络\"}"}' data-report-view='{"mod":"popu_626","spm":"1001.2101.3001.4223","strategy":"神经网络","ab":"new","extra":"{\"searchword\":\"神经网络\"}"}' class="tag-link" href="https://so.csdn.net/so/search/s.do?q=神经网络&amp;t=all&amp;o=vip&amp;s=&amp;l=&amp;f=&amp;viparticle=&amp;from_tracking_code=tag_word&amp;from_code=app_blog_art" target="_blank" rel="noopener">神经网络</a>
                    </div>
                </div>
                <div class="up-time"><span>于&nbsp;2022-07-26 12:37:04&nbsp;首次发布</span></div>
                <div class="slide-content-box">
                    <div class="article-copyright">
                        <div class="creativecommons">
                            版权声明：本文为博主原创文章，遵循<a href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank" rel="noopener"> CC 4.0 BY-SA </a>版权协议，转载请附上原文出处链接和本声明。
                        </div>
                        <div class="article-source-link">
                            本文链接：<a href="https://blog.csdn.net/qq_34758157/article/details/125990818" target="_blank">https://blog.csdn.net/qq_34758157/article/details/125990818</a>
                        </div>
                    </div>
                </div>
                
                <div class="operating">
                    <a class="href-article-edit slide-toggle">版权</a>
                </div>
            </div>
        </div>

    </div>
    <div id="blogHuaweiyunAdvert"></div>
    <article class="baidu_pl">
        <div id="article_content" class="article_content clearfix">
        <link rel="stylesheet" href="https://csdnimg.cn/release/blogv2/dist/mdeditor/css/editerView/kdoc_html_views-1a98987dfd.css">
        <link rel="stylesheet" href="https://csdnimg.cn/release/blogv2/dist/mdeditor/css/editerView/ck_htmledit_views-704d5b9767.css">
             
                <div id="content_views" class="markdown_views prism-tomorrow-night">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>关于pytorch中多个backward出现的问题&#xff1a;enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly True.</h4> 
</div> 
<p></p> 
<p>在执行代码中包含两个<strong>方向传播</strong>(backward)时&#xff0c;可能会出现这种问题&#xff1a;</p> 
<pre><code>RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [10, 1]], which is output 0 of AsStridedBackward0, is at version 2; expected version 1 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).
</code></pre> 
<p>什么情况下会出现这种问题&#xff0c;我们先构建一个<strong>场景</strong>&#xff1a;</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn <span class="token keyword">as</span> nn
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">import</span> functional <span class="token keyword">as</span> F
<span class="token keyword">from</span> torch <span class="token keyword">import</span> optim
</code></pre> 
<p>为了<strong>简化问题</strong>&#xff0c;构建两个<strong>相同的</strong>神经网络&#xff1a;</p> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">Net_1</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>Net_1<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>linear_1 <span class="token operator">&#61;</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">10</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>linear_2 <span class="token operator">&#61;</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">&#61;</span> self<span class="token punctuation">.</span>linear_1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">&#61;</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">&#61;</span> self<span class="token punctuation">.</span>linear_2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">&#61;</span> F<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>x<span class="token punctuation">,</span>dim<span class="token operator">&#61;</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> x

<span class="token keyword">class</span> <span class="token class-name">Net_2</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>Net_2<span class="token punctuation">,</span>self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>linear_1 <span class="token operator">&#61;</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">10</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>linear_2 <span class="token operator">&#61;</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">&#61;</span> self<span class="token punctuation">.</span>linear_1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">&#61;</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">&#61;</span> self<span class="token punctuation">.</span>linear_2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">&#61;</span> F<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>x<span class="token punctuation">,</span>dim<span class="token operator">&#61;</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> x
</code></pre> 
<p>算法<strong>执行流程</strong>&#xff1a;<br /> 定义模型<code>Net_1,Net_2</code>、两个模型对应的优化器(Optimizer)<code>optimizer_n1,optimizer_n2</code>&#xff0c;以及损失函数<code>criterion</code>&#xff1a;</p> 
<pre><code class="prism language-python">n_1 <span class="token operator">&#61;</span> Net_1<span class="token punctuation">(</span><span class="token punctuation">)</span>
n_2 <span class="token operator">&#61;</span> Net_2<span class="token punctuation">(</span><span class="token punctuation">)</span>

optimizer_n1 <span class="token operator">&#61;</span> optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>n_1<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>lr<span class="token operator">&#61;</span><span class="token number">0.001</span><span class="token punctuation">)</span>
optimizer_n2 <span class="token operator">&#61;</span> optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>n_2<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>lr<span class="token operator">&#61;</span><span class="token number">0.001</span><span class="token punctuation">)</span>
criterion <span class="token operator">&#61;</span> nn<span class="token punctuation">.</span>MSELoss<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<p>执行过程如下&#xff1a;</p> 
<pre><code class="prism language-python"><span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    x <span class="token operator">&#61;</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
    y <span class="token operator">&#61;</span> <span class="token number">2</span> <span class="token operator">*</span> x

    pred_n1 <span class="token operator">&#61;</span> n_1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
    optimizer_n1<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
    loss_n1 <span class="token operator">&#61;</span> criterion<span class="token punctuation">(</span>y<span class="token punctuation">,</span>pred_n1<span class="token punctuation">)</span>
    loss_n1<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
    optimizer_n1<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>

    pred_n2 <span class="token operator">&#61;</span> n_2<span class="token punctuation">(</span>pred_n1<span class="token punctuation">)</span>
    optimizer_n2<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
    loss_n2 <span class="token operator">&#61;</span> criterion<span class="token punctuation">(</span>y<span class="token punctuation">,</span>pred_n2<span class="token punctuation">)</span>
    loss_n2<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
    optimizer_n2<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<p>注意的点&#xff1a;该执行过程的特点是&#xff0c;第一个神经网络的<code>pred_n1</code>&#xff0c;它也参与了第二个神经网络的反向传播的过程。</p> 
<p>我们知道的是&#xff0c;<code>loss_n1.backward()</code>操作在执行后&#xff0c;计算节点被保存了&#xff0c;但是计算图结构被<strong>释放掉了</strong>&#xff0c;导致<strong>第二个损失函数</strong>进行<strong>反向传播过程中</strong>需要使用<strong>第一次反向传播的<mark>计算图结构</mark>失败</strong>。</p> 
<p>至此&#xff0c;我们在backward中使用<code>retain_graph&#61;True</code>来保存它的计算图结构&#xff1a;<br /> 这里有一<strong>小细节</strong>&#xff1a;<br /> 实际上<code>loss_n1,loss_n2</code>的<code>backward</code>都可以添加参数<code>retain_graph&#61;True</code>&#xff0c;我们之所以只在第一个里面添加&#xff0c;是因为<strong>如果第二个也加了&#xff0c;本次for循环中的计算图结构就堆积在了内存中&#xff0c;释放不掉&#xff0c;对内存是有负担的。所以一般情况下<mark>本次for循环的计算图使用完后&#xff0c;我们给它释放掉</mark></strong>。</p> 
<pre><code class="prism language-python"><span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    x <span class="token operator">&#61;</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
    y <span class="token operator">&#61;</span> <span class="token number">2</span> <span class="token operator">*</span> x

    pred_n1 <span class="token operator">&#61;</span> n_1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
    optimizer_n1<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
    loss_n1 <span class="token operator">&#61;</span> criterion<span class="token punctuation">(</span>y<span class="token punctuation">,</span>pred_n1<span class="token punctuation">)</span>
    loss_n1<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>retain_graph<span class="token operator">&#61;</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    optimizer_n1<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>

    pred_n2 <span class="token operator">&#61;</span> n_2<span class="token punctuation">(</span>pred_n1<span class="token punctuation">)</span>
    optimizer_n2<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
    loss_n2 <span class="token operator">&#61;</span> criterion<span class="token punctuation">(</span>y<span class="token punctuation">,</span>pred_n2<span class="token punctuation">)</span>
    loss_n2<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
    optimizer_n2<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<p>修改了这个细节之后&#xff0c;重新运行代码&#xff1a;<br /> <strong><mark>仍然在出错</mark></strong>。<br /> 这次出错的问题&#xff0c;就是标题的问题&#xff1a;<br /> 首先&#xff0c;按照它的要求&#xff0c;执行一次<code>torch.autograd.set_detect_anomaly(True)</code>&#xff1a;</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn <span class="token keyword">as</span> nn
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">import</span> functional <span class="token keyword">as</span> F
<span class="token keyword">from</span> torch <span class="token keyword">import</span> optim

torch<span class="token punctuation">.</span>autograd<span class="token punctuation">.</span>set_detect_anomaly<span class="token punctuation">(</span><span class="token boolean">True</span><span class="token punctuation">)</span>
</code></pre> 
<p>返回的报错结果如下&#xff1a;</p> 
<pre><code class="prism language-python">D<span class="token punctuation">:</span>\software\anaconda3\envs\pytorch\lib\site<span class="token operator">-</span>packages\torch\autograd\__init__<span class="token punctuation">.</span>py<span class="token punctuation">:</span><span class="token number">154</span><span class="token punctuation">:</span> UserWarning<span class="token punctuation">:</span> Error detected <span class="token keyword">in</span> AddmmBackward0<span class="token punctuation">.</span> Traceback of forward call that caused the error<span class="token punctuation">:</span>
  File <span class="token string">&#34;D:\code_work\reinforcement_learning\.pytest_cache\bark_test.py&#34;</span><span class="token punctuation">,</span> line <span class="token number">49</span><span class="token punctuation">,</span> <span class="token keyword">in</span> <span class="token operator">&lt;</span>module<span class="token operator">&gt;</span>
    pred_n1 <span class="token operator">&#61;</span> n_1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
  File <span class="token string">&#34;D:\software\anaconda3\envs\pytorch\lib\site-packages\torch\nn\modules\module.py&#34;</span><span class="token punctuation">,</span> line <span class="token number">1102</span><span class="token punctuation">,</span> <span class="token keyword">in</span> _call_impl
    <span class="token keyword">return</span> forward_call<span class="token punctuation">(</span><span class="token operator">*</span><span class="token builtin">input</span><span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span>
  File <span class="token string">&#34;D:\code_work\reinforcement_learning\.pytest_cache\bark_test.py&#34;</span><span class="token punctuation">,</span> line <span class="token number">18</span><span class="token punctuation">,</span> <span class="token keyword">in</span> forward
    x <span class="token operator">&#61;</span> self<span class="token punctuation">.</span>linear_2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
  File <span class="token string">&#34;D:\software\anaconda3\envs\pytorch\lib\site-packages\torch\nn\modules\module.py&#34;</span><span class="token punctuation">,</span> line <span class="token number">1102</span><span class="token punctuation">,</span> <span class="token keyword">in</span> _call_impl
    <span class="token keyword">return</span> forward_call<span class="token punctuation">(</span><span class="token operator">*</span><span class="token builtin">input</span><span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span>
  File <span class="token string">&#34;D:\software\anaconda3\envs\pytorch\lib\site-packages\torch\nn\modules\linear.py&#34;</span><span class="token punctuation">,</span> line <span class="token number">103</span><span class="token punctuation">,</span> <span class="token keyword">in</span> forward
    <span class="token keyword">return</span> F<span class="token punctuation">.</span>linear<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>weight<span class="token punctuation">,</span> self<span class="token punctuation">.</span>bias<span class="token punctuation">)</span>
  File <span class="token string">&#34;D:\software\anaconda3\envs\pytorch\lib\site-packages\torch\nn\functional.py&#34;</span><span class="token punctuation">,</span> line <span class="token number">1848</span><span class="token punctuation">,</span> <span class="token keyword">in</span> linear
    <span class="token keyword">return</span> torch<span class="token punctuation">.</span>_C<span class="token punctuation">.</span>_nn<span class="token punctuation">.</span>linear<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">,</span> weight<span class="token punctuation">,</span> bias<span class="token punctuation">)</span>
 <span class="token punctuation">(</span>Triggered internally at  <span class="token punctuation">.</span><span class="token punctuation">.</span>\torch\csrc\autograd\python_anomaly_mode<span class="token punctuation">.</span>cpp<span class="token punctuation">:</span><span class="token number">104.</span><span class="token punctuation">)</span>
  Variable<span class="token punctuation">.</span>_execution_engine<span class="token punctuation">.</span>run_backward<span class="token punctuation">(</span>
Traceback <span class="token punctuation">(</span>most recent call last<span class="token punctuation">)</span><span class="token punctuation">:</span>
  File <span class="token string">&#34;D:\code_work\reinforcement_learning\.pytest_cache\bark_test.py&#34;</span><span class="token punctuation">,</span> line <span class="token number">58</span><span class="token punctuation">,</span> <span class="token keyword">in</span> <span class="token operator">&lt;</span>module<span class="token operator">&gt;</span>
    loss_n2<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
  File <span class="token string">&#34;D:\software\anaconda3\envs\pytorch\lib\site-packages\torch\_tensor.py&#34;</span><span class="token punctuation">,</span> line <span class="token number">307</span><span class="token punctuation">,</span> <span class="token keyword">in</span> backward
    torch<span class="token punctuation">.</span>autograd<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>self<span class="token punctuation">,</span> gradient<span class="token punctuation">,</span> retain_graph<span class="token punctuation">,</span> create_graph<span class="token punctuation">,</span> inputs<span class="token operator">&#61;</span>inputs<span class="token punctuation">)</span>
  File <span class="token string">&#34;D:\software\anaconda3\envs\pytorch\lib\site-packages\torch\autograd\__init__.py&#34;</span><span class="token punctuation">,</span> line <span class="token number">154</span><span class="token punctuation">,</span> <span class="token keyword">in</span> backward
    Variable<span class="token punctuation">.</span>_execution_engine<span class="token punctuation">.</span>run_backward<span class="token punctuation">(</span>
RuntimeError<span class="token punctuation">:</span> one of the variables needed <span class="token keyword">for</span> gradient computation has been modified by an inplace operation<span class="token punctuation">:</span> <span class="token punctuation">[</span>torch<span class="token punctuation">.</span>FloatTensor <span class="token punctuation">[</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> which <span class="token keyword">is</span> output <span class="token number">0</span> of AsStridedBackward0<span class="token punctuation">,</span> <span class="token keyword">is</span> at version <span class="token number">2</span><span class="token punctuation">;</span> expected version <span class="token number">1</span> instead<span class="token punctuation">.</span> Hint<span class="token punctuation">:</span> the backtrace further above shows the operation that failed to compute its gradient<span class="token punctuation">.</span> The variable <span class="token keyword">in</span> question was changed <span class="token keyword">in</span> there <span class="token keyword">or</span> anywhere later<span class="token punctuation">.</span> Good luck!
</code></pre> 
<p>无论是前面<code>retain_graph&#61;True</code>报的错还是这个错误&#xff0c;都是指向<code>loss_n2.backward()</code>,上面的报错信息就详细地<strong>梳理了</strong><code>loss_n2</code><strong>反向传播的计算流程</strong>&#xff0c;和我们代码关联的只有一项&#xff1a;</p> 
<pre><code class="prism language-python">x <span class="token operator">&#61;</span> self<span class="token punctuation">.</span>linear_2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
</code></pre> 
<p>在这个计算过程中&#xff0c;由于存在<code>pred_n1</code>的参与&#xff0c;我们要保证<code>loss_n2.backward()</code>执行过程中<code>requires_grad&#61;False</code>&#xff0c;否则会发生冲突。<br /> 因此&#xff0c;我们在该神经网络中的<strong>初始部分</strong>使用<code>detach</code>操作&#xff0c;将它的<code>requires_grad</code>停止。即&#xff1a;</p> 
<pre><code class="prism language-python">x <span class="token operator">&#61;</span> self<span class="token punctuation">.</span>linear_1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<p><code>为什么只添加在初始位置&#xff0c;其余位置不加&#xff1f;</code><br /> 很简单&#xff0c;因为只有神经网络的初始节点(<mark><strong>叶节点</strong></mark>)才能接收梯度(<code>requires_grad&#61;True</code>)&#xff0c;<mark><strong>非叶节点</strong></mark>(隐藏层中的节点都是<code>requires_grad&#61;False</code>)<br /> 损失函数结果本身是<strong>标量</strong>&#xff0c;自然也不会有梯度的。<br /> 至此&#xff0c;上述错误解决。修改后完整代码如下&#xff0c;大家可以对比一下&#xff1a;</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn <span class="token keyword">as</span> nn
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">import</span> functional <span class="token keyword">as</span> F
<span class="token keyword">from</span> torch <span class="token keyword">import</span> optim


<span class="token keyword">class</span> <span class="token class-name">Net_1</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>Net_1<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>linear_1 <span class="token operator">&#61;</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">10</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>linear_2 <span class="token operator">&#61;</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">&#61;</span> self<span class="token punctuation">.</span>linear_1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">&#61;</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">&#61;</span> self<span class="token punctuation">.</span>linear_2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">&#61;</span> F<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>x<span class="token punctuation">,</span>dim<span class="token operator">&#61;</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> x


<span class="token keyword">class</span> <span class="token class-name">Net_2</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>Net_2<span class="token punctuation">,</span>self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>linear_1 <span class="token operator">&#61;</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">10</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>linear_2 <span class="token operator">&#61;</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">&#61;</span> self<span class="token punctuation">.</span>linear_1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span>
        x <span class="token operator">&#61;</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">&#61;</span> self<span class="token punctuation">.</span>linear_2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">&#61;</span> F<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>x<span class="token punctuation">,</span>dim<span class="token operator">&#61;</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> x


n_1 <span class="token operator">&#61;</span> Net_1<span class="token punctuation">(</span><span class="token punctuation">)</span>
n_2 <span class="token operator">&#61;</span> Net_2<span class="token punctuation">(</span><span class="token punctuation">)</span>

optimizer_n1 <span class="token operator">&#61;</span> optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>n_1<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>lr<span class="token operator">&#61;</span><span class="token number">0.001</span><span class="token punctuation">)</span>
optimizer_n2 <span class="token operator">&#61;</span> optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>n_2<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>lr<span class="token operator">&#61;</span><span class="token number">0.001</span><span class="token punctuation">)</span>
criterion <span class="token operator">&#61;</span> nn<span class="token punctuation">.</span>MSELoss<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    x <span class="token operator">&#61;</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
    y <span class="token operator">&#61;</span> <span class="token number">2</span> <span class="token operator">*</span> x

    pred_n1 <span class="token operator">&#61;</span> n_1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
    optimizer_n1<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
    loss_n1 <span class="token operator">&#61;</span> criterion<span class="token punctuation">(</span>y<span class="token punctuation">,</span>pred_n1<span class="token punctuation">)</span>
    loss_n1<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>retain_graph<span class="token operator">&#61;</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    optimizer_n1<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>

    pred_n2 <span class="token operator">&#61;</span> n_2<span class="token punctuation">(</span>pred_n1<span class="token punctuation">)</span>
    optimizer_n2<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
    loss_n2 <span class="token operator">&#61;</span> criterion<span class="token punctuation">(</span>y<span class="token punctuation">,</span>pred_n2<span class="token punctuation">)</span>
    loss_n2<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
    optimizer_n2<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>

</code></pre>
                </div>
                <link href="https://csdnimg.cn/release/blogv2/dist/mdeditor/css/editerView/markdown_views-f23dff6052.css" rel="stylesheet">
                <link href="https://csdnimg.cn/release/blogv2/dist/mdeditor/css/style-e504d6a974.css" rel="stylesheet">
        </div>
    </article>
  <script>
    $(function() {
      setTimeout(function () {
        var mathcodeList = document.querySelectorAll('.htmledit_views img.mathcode');
        if (mathcodeList.length > 0) {
          for (let i = 0; i < mathcodeList.length; i  ) {
            if (mathcodeList[i].complete) {
              if (mathcodeList[i].naturalWidth === 0 || mathcodeList[i].naturalHeight === 0) {
                var alt = mathcodeList[i].alt;
                alt = '\\('   alt   '\\)';
                var curSpan = $('<span class="img-codecogs"></span>');
                curSpan.text(alt);
                $(mathcodeList[i]).before(curSpan);
                $(mathcodeList[i]).remove();
              }
            } else {
              mathcodeList[i].onerror = function() {
                var alt = mathcodeList[i].alt;
                alt = '\\('   alt   '\\)';
                var curSpan = $('<span class="img-codecogs"></span>');
                curSpan.text(alt);
                $(mathcodeList[i]).before(curSpan);
                $(mathcodeList[i]).remove();
              };
            }
          }
          MathJax.Hub.Queue(["Typeset",MathJax.Hub]);
        }
      }, 500)
    });
  </script>
</div>
<div class="directory-boxshadow-dialog" style="display:none;">
  <div class="directory-boxshadow-dialog-box">
  </div>
   <div class="vip-limited-time-offer-box-new" id="vip-limited-time-offer-box-new">
      <img class="limited-img limited-img-new" src="https://csdnimg.cn/release/blogv2/dist/pc/img/vip-limited-close-newWhite.png">
      <div class="vip-limited-time-top">
        确定要放弃本次机会？
      </div>
      <span class="vip-limited-time-text">福利倒计时</span>
      <div class="limited-time-box-new">
        <span class="time-hour"></span>
        <i>:</i>
        <span class="time-minite"></span>
        <i>:</i>
        <span class="time-second"></span>
      </div>
      <div class="limited-time-vip-box">
        <p>
          <img class="coupon-img" src="https://csdnimg.cn/release/blogv2/dist/pc/img/vip-limited-close-roup.png">
          <span class="def">立减 yen</span>
          <span class="active limited-num"></span>
        </p>
        <span class="">普通VIP年卡可用</span>
      </div>
      <a class="limited-time-btn-new" href="https://mall.csdn.net/vip" data-report-click='{"spm":"1001.2101.3001.9621"}' data-report-query='spm=1001.2101.3001.9621'>立即使用</a>
  </div>
</div>    <div class="more-toolbox-new more-toolbar" id="toolBarBox">
      <div class="left-toolbox">
        <div class="toolbox-left">
            <div class="profile-box">
              <a class="profile-href" target="_blank" href="https://xiaowutongzhi.blog.csdn.net"><img class="profile-img" src="https://profile-avatar.csdnimg.cn/ce7b07d1d02f4b7890f10fc0dfa748bf_qq_34758157.jpg!1">
                <span class="profile-name">
                  静静的喝酒
                </span>
              </a>
            </div>
            <div class="profile-attend">
                <a class="tool-attend tool-bt-button tool-bt-attend" href="javascript:;" data-report-view='{"mod":"1592215036_002","spm":"1001.2101.3001.4232","extend1":"关注"}'>关注</a>
              <a class="tool-item-follow active-animation" style="display:none;">关注</a>
            </div>
        </div>
        <div class="toolbox-middle">
          <ul class="toolbox-list">
            <li class="tool-item tool-item-size tool-active is-like" id="is-like">
              <a class="tool-item-href">
                <img style="display:none;" id="is-like-imgactive-animation-like" class="animation-dom active-animation" src="https://csdnimg.cn/release/blogv2/dist/pc/img/tobarThumbUpactive.png" alt="">
                <img class="isactive" style="display:none" id="is-like-imgactive" src="https://csdnimg.cn/release/blogv2/dist/pc/img/toolbar/like-active.png" alt="">
                <img class="isdefault" style="display:block" id="is-like-img" src="https://csdnimg.cn/release/blogv2/dist/pc/img/toolbar/like.png" alt="">
                <span id="spanCount" class="count ">
                    31
                </span>
              </a>
              <div class="tool-hover-tip"><span class="text space">点赞</span></div>
            </li>
            <li class="tool-item tool-item-size tool-active is-unlike" id="is-unlike">
              <a class="tool-item-href">
                <img class="isactive" style="margin-right:0px;display:none" id="is-unlike-imgactive" src="https://csdnimg.cn/release/blogv2/dist/pc/img/toolbar/unlike-active.png" alt="">
                <img class="isdefault" style="margin-right:0px;display:block" id="is-unlike-img" src="https://csdnimg.cn/release/blogv2/dist/pc/img/toolbar/unlike.png" alt="">
                <span id="unlikeCount" class="count "></span>
              </a>
              <div class="tool-hover-tip"><span class="text space">踩</span></div>
            </li>
            <li class="tool-item tool-item-size tool-active is-collection ">
              <a class="tool-item-href" href="javascript:;" data-report-click='{"mod":"popu_824","spm":"1001.2101.3001.4130","ab":"new"}'>
                <img style="display:none" id="is-collection-img-collection" class="animation-dom active-animation" src="https://csdnimg.cn/release/blogv2/dist/pc/img/toolbar/collect-active.png" alt="">
                <img class="isdefault" id="is-collection-img" style="display:block" src="https://csdnimg.cn/release/blogv2/dist/pc/img/toolbar/collect.png" alt="">
                <img class="isactive" id="is-collection-imgactive" style="display:none" src="https://csdnimg.cn/release/blogv2/dist/pc/img/newCollectActive.png" alt="">
                <span class="count get-collection " data-num="43" id="get-collection">
                    43
                </span>
              </a>
              <div class="tool-hover-tip collect">
                <div class="collect-operate-box">
                  <span class="collect-text" id="is-collection">
                    收藏
                  </span>
                </div>
              </div>
              <div class="tool-active-list">
                <div class="text">
                  觉得还不错?
                  <span class="collect-text" id="tool-active-list-collection">
                    一键收藏
                  </span>
                 <img id="tool-active-list-close" src="https://csdnimg.cn/release/blogv2/dist/pc/img/collectionCloseWhite.png" alt="">
                </div>
              </div>
            </li>
            <li class="tool-item tool-item-size tool-active tool-item-comment">
              <div class="guide-rr-first">
                <img src="https://csdnimg.cn/release/blogv2/dist/pc/img/guideRedReward01.png" alt="">
                <button class="btn-guide-known">知道了</button>
              </div>
                <a class="tool-item-href go-side-comment" data-report-click='{"spm":"1001.2101.3001.7009"}'>
                <img class="isdefault" src="https://csdnimg.cn/release/blogv2/dist/pc/img/toolbar/comment.png" alt="">
                <span class="count">
                    5
                </span>
              </a>
              <div class="tool-hover-tip"><span class="text space">评论</span></div>
            </li>
            <li class="tool-item tool-item-size tool-active tool-QRcode" data-type="article" id="tool-share">
              <a class="tool-item-href" href="javascript:;" data-report-view='{"spm":"3001.4129","extra":{"type":"blogdetail"}}'>
                <img class="isdefault" src="https://csdnimg.cn/release/blogv2/dist/pc/img/toolbar/share.png" alt="">
                <span class="count">分享</span>
              </a>
                <div class="QRcode active" id="tool-QRcode">
                <div class="share-bg-box">
                  <div class="share-content">
                    <a id="copyPosterUrl" data-type="link" class="btn-share">复制链接</a>
                  </div>
                  <div class="share-content">
                    <a class="btn-share" data-type="qq">分享到 QQ</a>
                  </div>
                  <div class="share-content">
                    <a class="btn-share" data-type="weibo">分享到新浪微博</a>
                  </div>
                  <div class="share-code">
                    <div class="share-code-box" id='shareCode'></div>
                    <div class="share-code-text">
                      <img src="https://csdnimg.cn/release/blogv2/dist/pc/img/share/icon-wechat.png" alt="">扫一扫
                    </div>
                  </div>
                </div>
              </div>
            </li>
                <li class="tool-item tool-item-size tool-active tool-item-reward">
                  <a class="tool-item-href" href="javascript:;" data-report-click='{"mod":"popu_830","spm":"1001.2101.3001.4237","dest":"","ab":"new"}'>
                    <img class="isdefault reward-bt" id="rewardBtNew" src="https://csdnimg.cn/release/blogv2/dist/pc/img/toolbar/reward.png" alt="打赏">
                    <span class="count">打赏</span>
                  </a>
                  <div class="tool-hover-tip"><span class="text space">打赏</span></div>
                </li>
          <li class="tool-item tool-item-size tool-active is-more" id="is-more">
            <a class="tool-item-href">
              <img class="isdefault" style="margin-right:0px;display:block" src="https://csdnimg.cn/release/blogv2/dist/pc/img/toolbar/more.png" alt="">
              <span class="count"></span>
            </a>
            <div class="more-opt-box">
              <div class="mini-box">
                    <a class="tool-item-href" id="rewardBtNewHide" data-report-click='{"spm":"3001.4237","extra":"{\"type\":\"hide\"}"}'>
                      <img class="isdefault reward-bt" src="https://csdnimg.cn/release/blogv2/dist/pc/img/toolbar/reward.png" alt="打赏">
                      <span class="count">打赏</span>
                    </a>
                <a class="tool-item-href" id="toolReportBtnHide">
                  <img class="isdefault" src="https://csdnimg.cn/release/blogv2/dist/pc/img/toolbar/report.png" alt="">
                  <span class="count">举报</span>
                </a>
              </div>
              <div class="normal-box">
                <a class="tool-item-href" id="toolReportBtnHideNormal">
                  <img class="isdefault" src="https://csdnimg.cn/release/blogv2/dist/pc/img/toolbar/report.png" alt="">
                  <span class="count">举报</span>
                </a>
              </div>
            </div>
          </li>
        </ul>
      </div>
      <div class="toolbox-right"><script type="text/javascript" crossorigin src="https://g.csdnimg.cn/common/csdn-login-box/csdn-login-box.js"></script></html>